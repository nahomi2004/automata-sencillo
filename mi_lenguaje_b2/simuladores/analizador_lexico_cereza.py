import re

# === üçí Analizador L√©xico para el Lenguaje Cereza ===

RESERVED = {"var", "if", "while"}

BAD_OPERATORS = {"=>", "=<", "==>", "<==", "===", "!==", "->", "<-", "><"}
SEPARATED_EQ = re.compile(r"= =")

TOKEN_REGEX = [
    ("COMMENT_LINE", r"#.*"),
    ("NUMBER", r"-?\d+"),
    ("EQ", r"=="),
    ("NEQ", r"!="),
    ("GE", r">="),
    ("LE", r"<="),
    ("GT", r">"),
    ("LT", r"<"),
    ("PLUS", r"\+"),
    ("MINUS", r"-"),
    ("MULT", r"\*"),
    ("DIV", r"/"),
    ("ASSIGN", r"="),
    ("LBRACE", r"\{"),
    ("RBRACE", r"\}"),
    ("LPAREN", r"\("),
    ("RPAREN", r"\)"),
    ("ID", r"[a-zA-Z_][a-zA-Z_0-9]*"),
    ("WS", r"\s+"),
    ("UNKNOWN", r".")
]

TOKEN_RE = re.compile("|".join(f"(?P<{name}>{pattern})" for name, pattern in TOKEN_REGEX))

def tokenize_line(line, lineno):
    tokens = []

    if line.strip().endswith(";"):
        return [("‚ùå ERROR", lineno, "; al final de l√≠nea no es v√°lido")]

    if SEPARATED_EQ.search(line):
        return [("‚ùå ERROR", lineno, "Uso de '= =' separado. Use '=='")]

    for bad in BAD_OPERATORS:
        if bad in line:
            return [("‚ùå ERROR", lineno, f"Operador mal formado '{bad}'")]

    pos = 0
    while pos < len(line):
        match = TOKEN_RE.match(line, pos)
        if not match:
            tokens.append(("‚ùå ERROR", lineno, f"S√≠mbolo no reconocido: '{line[pos]}'"))
            pos += 1
            continue

        kind = match.lastgroup
        value = match.group()
        if kind == "WS":
            pos = match.end()
            continue

        if kind == "UNKNOWN":
            tokens.append(("‚ùå ERROR", lineno, f"Car√°cter no v√°lido: '{value}'"))
        elif kind == "ID" and value in RESERVED:
            tokens.append(("‚úÖ RESERVED", value))
        elif kind == "ID":
            tokens.append(("‚úÖ IDENTIFIER", value))
        else:
            tokens.append(("‚úÖ", kind, value))

        pos = match.end()

    return tokens

def analizador_lexico_cereza(texto):
    resultados = []
    lineas = texto.splitlines()
    for num, linea in enumerate(lineas, 1):
        if linea.strip().startswith("#") or not linea.strip():
            continue
        resultado = tokenize_line(linea, num)
        resultados.append((num, resultado))
    return resultados
