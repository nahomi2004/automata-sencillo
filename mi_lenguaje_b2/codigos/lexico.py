import re

# === üçí Analizador L√©xico para el Lenguaje Cereza ===

RESERVED = {"var", "if", "while"}

BAD_OPERATORS = {"=>", "=<", "==>", "<==", "===", "!==", "->", "<-", "><"}
SEPARATED_EQ = re.compile(r"= =")  # para detectar '= ='

# Solo tokens que existen en tu gram√°tica actual:
TOKEN_REGEX = [
    ("COMMENT_LINE", r"#.*"),
    ("NUMBER", r"-?\d+"),
    ("EQ", r"=="),
    ("NEQ", r"!="),
    ("GT", r">"),
    ("LT", r"<"),
    ("PLUS", r"\+"),
    ("MINUS", r"-"),
    ("MULT", r"\*"),
    ("DIV", r"/"),
    ("ASSIGN", r"="),
    ("LBRACE", r"\{"),
    ("RBRACE", r"\}"),
    ("LPAREN", r"\("),
    ("RPAREN", r"\)"),
    ("ID", r"[a-zA-Z_][a-zA-Z_0-9]*"),
    ("WS", r"\s+"),
    ("UNKNOWN", r".")
]

TOKEN_RE = re.compile("|".join(f"(?P<{name}>{pattern})" for name, pattern in TOKEN_REGEX))

def tokenize_line(line, lineno):
    tokens = []

    # Validar fin de l√≠nea con ';' no permitido seg√∫n gram√°tica
    if line.strip().endswith(";"):
        return [("‚ùå ERROR", lineno, "; al final de l√≠nea no es v√°lido")]

    # Verificar que no haya '= =' separado, debe ser '=='
    if SEPARATED_EQ.search(line):
        return [("‚ùå ERROR", lineno, "Uso de '= =' separado. Use '=='")]

    # Revisar operadores mal formados prohibidos
    for bad in BAD_OPERATORS:
        if bad in line:
            return [("‚ùå ERROR", lineno, f"Operador mal formado '{bad}'")]

    pos = 0
    while pos < len(line):
        match = TOKEN_RE.match(line, pos)
        if not match:
            tokens.append(("‚ùå ERROR", lineno, f"S√≠mbolo no reconocido: '{line[pos]}'"))
            pos += 1
            continue

        kind = match.lastgroup
        value = match.group()

        # Ignorar espacios
        if kind == "WS":
            pos = match.end()
            continue

        # Ignorar l√≠neas de comentario, no deben llegar aqu√≠ porque se omiten antes
        if kind == "COMMENT_LINE":
            pos = match.end()
            continue

        # Detectar caracteres desconocidos
        if kind == "UNKNOWN":
            tokens.append(("‚ùå ERROR", lineno, f"Car√°cter no v√°lido: '{value}'"))
        elif kind == "ID" and value in RESERVED:
            tokens.append(("‚úÖ RESERVED", value))
        elif kind == "ID":
            tokens.append(("‚úÖ IDENTIFIER", value))
        else:
            tokens.append(("‚úÖ", kind, value))

        pos = match.end()

    return tokens

def analizador_lexico_cereza(texto):
    resultados = []
    lineas = texto.splitlines()
    for num, linea in enumerate(lineas, 1):
        linea_strip = linea.strip()
        # Omitir l√≠neas vac√≠as y l√≠neas que inician con #
        if linea_strip.startswith("#") or not linea_strip:
            continue
        resultado = tokenize_line(linea_strip, num)
        
        # Agrega token NEWLINE al final de cada l√≠nea v√°lida  
        resultado.append(("NEWLINE", "\\n"))
        resultados.append((num, resultado))
    return resultados